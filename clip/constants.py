
EPOCHS = 10
BATCH_SIZE = 32
MICROBATCH_SIZE = 4 # If batch size is too much for mem

# Size of encodings
LATENT_DIM = 2048

# Size of GPT embedding (n_feat from gpt2)
GPT_OUT = 784

# training
LEARNING_RATE = 1e-4
