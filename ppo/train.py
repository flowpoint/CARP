import torch
from torch import nn
import torch.nn.functional as F

from constants import * # CAPITAL vars are constants

# Take CLIP model, text generated by model and
# critic (review text)
def clip_reward(clip_model, generated_tokens, critic):
    passage_embedding = clip_model.encodeX(generated_tokens).squeeze()
    review_embedding = clip_model.encodeY(critic).squeeze()

    sim = F.consine_similarity(passage_embedding, review_embedding)
    # Reward on [-1, 1] should be good
    return sim

def kl_reward(new_log_probs, baseline_log_probs):
    kl_divs = (new_log_probs - old_log_probs) * torch.exp(new_log_probs)
    kl_divs = torch.sum(kl_divs, dim = 1)
    return kl_divs.mean()

# PPO loss function
def ppo_loss(model, state_batch, action_batch, action_log_probs, adv_batch, reward_batch):
    probs, new_value = model(state_batch)
    new_log_probs = torch.log(probs)
    
    ratio = (new_log_probs - action_log_probs).exp()
    surr1 = ratio * adv_batch
    surr2 = torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON) * adv_batch

    actor_loss = -torch.min(surr1, surr2)
    critic_loss = (new_value - reward_batch).pow(2).mean() # MSE

    loss = actor_loss + CRITIC_COEFF * critic_loss
    return loss

# Runs a single train stage over rollout
def train_PPO(model, optimizer, rollout_store):
    # NOTE: Not yet sure on what to actually put in rollout

    states, actions, log_probs, values, rewards = rollout_store.unwind()
    advantages = rewards - values
    advantages = (advatages - advantages.mean()) / (advantages.std() + 1e-8)

    size = len(states)
    total_loss = 0

    for epoch in range(EPOCHS_PER_ROLLOUT):
        # Shuffle indices
        inds = torch.randperm(size).split(BATCH_SIZE)
        for ind in inds:
            loss = ppo_loss(model, states[ind], actions[ind], log_probs[ind],
                    advantages[ind], rewards[ind])
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
    return total_loss
