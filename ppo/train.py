import torch
from torch import nn
import torch.nn.functional as F

from constants import * # CAPITAL vars are constants
import util

# Take CLIP model, text generated by model and
# critic (review text)
def clip_reward(clip_model, generated_tokens, critic):
    passage_embedding = clip_model.encodeX(generated_tokens).squeeze()
    review_embedding = clip_model.encodeY(critic).squeeze()

    sim = F.consine_similarity(passage_embedding, review_embedding)
    # Reward on [-1, 1] should be good
    return sim

def kl_reward(new_log_probs, baseline_log_probs):
    kl_divs = (new_log_probs - old_log_probs) * torch.exp(new_log_probs)
    kl_divs = torch.sum(kl_divs, dim = 1)
    return kl_divs.mean()

# PPO loss function
def ppo_loss(model, state_batch, action_batch, action_log_probs, adv_batch, reward_batch):
    probs, new_value = model(state_batch)

    # Finds probabilities of new model taking the actions
    # the old one took
    new_log_probs = probs.gather(1, action_batch.unsqueeze(1))
    new_log_probs = new_log_probs.squeeze()
    new_log_probs = torch.log(new_log_probs)

    ratio = (new_log_probs - action_log_probs).exp()
    surr1 = ratio * adv_batch
    surr2 = torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON) * adv_batch

    actor_loss = (-torch.min(surr1, surr2)).mean()
    critic_loss = (new_value - reward_batch).pow(2).mean() # MSE

    loss = actor_loss + CRITIC_COEFF * critic_loss
    return loss

# Runs a single train stage over rollout
def train_PPO(model, optimizer, rollout_store):
    # NOTE: Not yet sure on what to actually put in rollout

    log_probs, values, states, actions, rewards, _ = rollout_store.unwind()
    
    # For some reason rewards keep getting put on autograd
    # NOTE: Maybe specific to gym environments?
    rewards = rewards.detach().squeeze()
    
    advantages = util.normalize(rewards - values)

    size = len(states)
    total_loss = 0

    model.train()
    for epoch in range(EPOCHS_PER_ROLLOUT):
        # Shuffle indices
        if BATCH_SIZE == -1:
            inds = [torch.randperm(size)]
        else:
            inds = util.generate_indices(size, BATCH_SIZE)
        for ind in inds:
            optimizer.zero_grad()
            
            loss = ppo_loss(model, states[ind], actions[ind], log_probs[ind],
                    advantages[ind], rewards[ind])
            total_loss += loss.item()

            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
            optimizer.step()
    return total_loss
